{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d39306cb-55ff-454f-a703-c391e0fb7ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#!pip install xgboost\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "#!pip install shap\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "562f7524-2600-4db0-9453-3f23864f5887",
   "metadata": {},
   "outputs": [],
   "source": [
    "fTest = pd.read_csv('Resources/test.csv')\n",
    "dfTrain=pd.read_csv('Resources/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e0bfeb8-17dc-460e-a046-9a858b8514cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_code</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>...</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>190358</th>\n",
       "      <td>test_190358</td>\n",
       "      <td>11.2498</td>\n",
       "      <td>2.2982</td>\n",
       "      <td>6.6887</td>\n",
       "      <td>9.3982</td>\n",
       "      <td>8.6987</td>\n",
       "      <td>-9.9887</td>\n",
       "      <td>6.3711</td>\n",
       "      <td>20.4580</td>\n",
       "      <td>4.1432</td>\n",
       "      <td>...</td>\n",
       "      <td>2.3685</td>\n",
       "      <td>10.2977</td>\n",
       "      <td>2.7893</td>\n",
       "      <td>0.2810</td>\n",
       "      <td>15.3339</td>\n",
       "      <td>0.5334</td>\n",
       "      <td>-2.7295</td>\n",
       "      <td>8.2777</td>\n",
       "      <td>18.5502</td>\n",
       "      <td>4.6184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47302</th>\n",
       "      <td>test_47302</td>\n",
       "      <td>10.3755</td>\n",
       "      <td>-3.9807</td>\n",
       "      <td>12.4580</td>\n",
       "      <td>7.4824</td>\n",
       "      <td>12.5221</td>\n",
       "      <td>-15.2054</td>\n",
       "      <td>5.1038</td>\n",
       "      <td>16.8038</td>\n",
       "      <td>-3.8488</td>\n",
       "      <td>...</td>\n",
       "      <td>3.6439</td>\n",
       "      <td>13.2041</td>\n",
       "      <td>0.5700</td>\n",
       "      <td>4.6744</td>\n",
       "      <td>13.5791</td>\n",
       "      <td>-0.3281</td>\n",
       "      <td>6.7513</td>\n",
       "      <td>9.6446</td>\n",
       "      <td>20.2465</td>\n",
       "      <td>-17.8792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191036</th>\n",
       "      <td>test_191036</td>\n",
       "      <td>7.7884</td>\n",
       "      <td>-3.3308</td>\n",
       "      <td>7.3337</td>\n",
       "      <td>7.4973</td>\n",
       "      <td>8.7962</td>\n",
       "      <td>-0.0462</td>\n",
       "      <td>4.7501</td>\n",
       "      <td>7.9795</td>\n",
       "      <td>2.2902</td>\n",
       "      <td>...</td>\n",
       "      <td>1.1875</td>\n",
       "      <td>8.3446</td>\n",
       "      <td>2.9491</td>\n",
       "      <td>6.2438</td>\n",
       "      <td>13.8652</td>\n",
       "      <td>0.8077</td>\n",
       "      <td>-2.3237</td>\n",
       "      <td>9.1202</td>\n",
       "      <td>17.2514</td>\n",
       "      <td>-9.0226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134505</th>\n",
       "      <td>test_134505</td>\n",
       "      <td>14.7070</td>\n",
       "      <td>-2.0041</td>\n",
       "      <td>13.3141</td>\n",
       "      <td>2.9479</td>\n",
       "      <td>11.9246</td>\n",
       "      <td>-3.0086</td>\n",
       "      <td>5.6383</td>\n",
       "      <td>16.5664</td>\n",
       "      <td>0.9780</td>\n",
       "      <td>...</td>\n",
       "      <td>4.2571</td>\n",
       "      <td>7.7794</td>\n",
       "      <td>2.4471</td>\n",
       "      <td>8.8562</td>\n",
       "      <td>22.3861</td>\n",
       "      <td>0.9373</td>\n",
       "      <td>3.8026</td>\n",
       "      <td>10.0583</td>\n",
       "      <td>18.7704</td>\n",
       "      <td>-4.3610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172632</th>\n",
       "      <td>test_172632</td>\n",
       "      <td>13.2915</td>\n",
       "      <td>-7.3799</td>\n",
       "      <td>12.8063</td>\n",
       "      <td>6.1045</td>\n",
       "      <td>10.7534</td>\n",
       "      <td>3.2665</td>\n",
       "      <td>6.2919</td>\n",
       "      <td>18.6028</td>\n",
       "      <td>-4.3904</td>\n",
       "      <td>...</td>\n",
       "      <td>5.1984</td>\n",
       "      <td>8.2667</td>\n",
       "      <td>3.3268</td>\n",
       "      <td>7.1909</td>\n",
       "      <td>21.1988</td>\n",
       "      <td>2.1164</td>\n",
       "      <td>8.4392</td>\n",
       "      <td>9.7136</td>\n",
       "      <td>14.5550</td>\n",
       "      <td>-0.3629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174810</th>\n",
       "      <td>test_174810</td>\n",
       "      <td>11.5788</td>\n",
       "      <td>1.3556</td>\n",
       "      <td>9.7852</td>\n",
       "      <td>6.7871</td>\n",
       "      <td>11.9020</td>\n",
       "      <td>-8.7722</td>\n",
       "      <td>6.2753</td>\n",
       "      <td>19.5641</td>\n",
       "      <td>-4.3958</td>\n",
       "      <td>...</td>\n",
       "      <td>2.6253</td>\n",
       "      <td>6.6561</td>\n",
       "      <td>2.5417</td>\n",
       "      <td>3.7864</td>\n",
       "      <td>15.7055</td>\n",
       "      <td>-0.0573</td>\n",
       "      <td>-0.4335</td>\n",
       "      <td>8.9254</td>\n",
       "      <td>15.5573</td>\n",
       "      <td>18.2979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183503</th>\n",
       "      <td>test_183503</td>\n",
       "      <td>10.3949</td>\n",
       "      <td>-6.3226</td>\n",
       "      <td>10.5499</td>\n",
       "      <td>6.3926</td>\n",
       "      <td>12.0193</td>\n",
       "      <td>-15.1864</td>\n",
       "      <td>5.7223</td>\n",
       "      <td>10.8852</td>\n",
       "      <td>-0.3917</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.6850</td>\n",
       "      <td>8.0090</td>\n",
       "      <td>1.0291</td>\n",
       "      <td>1.7049</td>\n",
       "      <td>22.3925</td>\n",
       "      <td>1.6333</td>\n",
       "      <td>-2.3253</td>\n",
       "      <td>8.5062</td>\n",
       "      <td>13.7761</td>\n",
       "      <td>-10.7626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112986</th>\n",
       "      <td>test_112986</td>\n",
       "      <td>13.1623</td>\n",
       "      <td>2.4830</td>\n",
       "      <td>10.5983</td>\n",
       "      <td>6.9147</td>\n",
       "      <td>11.7334</td>\n",
       "      <td>-14.2995</td>\n",
       "      <td>4.5785</td>\n",
       "      <td>12.5001</td>\n",
       "      <td>-2.1497</td>\n",
       "      <td>...</td>\n",
       "      <td>5.5684</td>\n",
       "      <td>5.1732</td>\n",
       "      <td>1.5058</td>\n",
       "      <td>1.9068</td>\n",
       "      <td>17.5139</td>\n",
       "      <td>-2.1711</td>\n",
       "      <td>8.4400</td>\n",
       "      <td>8.4767</td>\n",
       "      <td>10.3667</td>\n",
       "      <td>7.9010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163611</th>\n",
       "      <td>test_163611</td>\n",
       "      <td>8.4372</td>\n",
       "      <td>4.0465</td>\n",
       "      <td>11.9709</td>\n",
       "      <td>6.7618</td>\n",
       "      <td>11.0751</td>\n",
       "      <td>-11.9672</td>\n",
       "      <td>6.0257</td>\n",
       "      <td>14.7172</td>\n",
       "      <td>1.6133</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.6306</td>\n",
       "      <td>2.0231</td>\n",
       "      <td>2.5323</td>\n",
       "      <td>-2.2266</td>\n",
       "      <td>16.8322</td>\n",
       "      <td>-2.2593</td>\n",
       "      <td>-4.0647</td>\n",
       "      <td>8.2448</td>\n",
       "      <td>19.3697</td>\n",
       "      <td>1.3178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15021</th>\n",
       "      <td>test_15021</td>\n",
       "      <td>6.7048</td>\n",
       "      <td>-0.2363</td>\n",
       "      <td>5.6569</td>\n",
       "      <td>6.5135</td>\n",
       "      <td>10.9319</td>\n",
       "      <td>0.0123</td>\n",
       "      <td>5.0597</td>\n",
       "      <td>16.4948</td>\n",
       "      <td>3.9969</td>\n",
       "      <td>...</td>\n",
       "      <td>9.6760</td>\n",
       "      <td>8.9115</td>\n",
       "      <td>4.3486</td>\n",
       "      <td>1.6985</td>\n",
       "      <td>21.0034</td>\n",
       "      <td>0.1780</td>\n",
       "      <td>9.3603</td>\n",
       "      <td>7.8386</td>\n",
       "      <td>17.0758</td>\n",
       "      <td>-8.7806</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID_code    var_0   var_1    var_2   var_3    var_4    var_5  \\\n",
       "190358  test_190358  11.2498  2.2982   6.6887  9.3982   8.6987  -9.9887   \n",
       "47302    test_47302  10.3755 -3.9807  12.4580  7.4824  12.5221 -15.2054   \n",
       "191036  test_191036   7.7884 -3.3308   7.3337  7.4973   8.7962  -0.0462   \n",
       "134505  test_134505  14.7070 -2.0041  13.3141  2.9479  11.9246  -3.0086   \n",
       "172632  test_172632  13.2915 -7.3799  12.8063  6.1045  10.7534   3.2665   \n",
       "...             ...      ...     ...      ...     ...      ...      ...   \n",
       "174810  test_174810  11.5788  1.3556   9.7852  6.7871  11.9020  -8.7722   \n",
       "183503  test_183503  10.3949 -6.3226  10.5499  6.3926  12.0193 -15.1864   \n",
       "112986  test_112986  13.1623  2.4830  10.5983  6.9147  11.7334 -14.2995   \n",
       "163611  test_163611   8.4372  4.0465  11.9709  6.7618  11.0751 -11.9672   \n",
       "15021    test_15021   6.7048 -0.2363   5.6569  6.5135  10.9319   0.0123   \n",
       "\n",
       "         var_6    var_7   var_8  ...  var_190  var_191  var_192  var_193  \\\n",
       "190358  6.3711  20.4580  4.1432  ...   2.3685  10.2977   2.7893   0.2810   \n",
       "47302   5.1038  16.8038 -3.8488  ...   3.6439  13.2041   0.5700   4.6744   \n",
       "191036  4.7501   7.9795  2.2902  ...   1.1875   8.3446   2.9491   6.2438   \n",
       "134505  5.6383  16.5664  0.9780  ...   4.2571   7.7794   2.4471   8.8562   \n",
       "172632  6.2919  18.6028 -4.3904  ...   5.1984   8.2667   3.3268   7.1909   \n",
       "...        ...      ...     ...  ...      ...      ...      ...      ...   \n",
       "174810  6.2753  19.5641 -4.3958  ...   2.6253   6.6561   2.5417   3.7864   \n",
       "183503  5.7223  10.8852 -0.3917  ...  -3.6850   8.0090   1.0291   1.7049   \n",
       "112986  4.5785  12.5001 -2.1497  ...   5.5684   5.1732   1.5058   1.9068   \n",
       "163611  6.0257  14.7172  1.6133  ...  -3.6306   2.0231   2.5323  -2.2266   \n",
       "15021   5.0597  16.4948  3.9969  ...   9.6760   8.9115   4.3486   1.6985   \n",
       "\n",
       "        var_194  var_195  var_196  var_197  var_198  var_199  \n",
       "190358  15.3339   0.5334  -2.7295   8.2777  18.5502   4.6184  \n",
       "47302   13.5791  -0.3281   6.7513   9.6446  20.2465 -17.8792  \n",
       "191036  13.8652   0.8077  -2.3237   9.1202  17.2514  -9.0226  \n",
       "134505  22.3861   0.9373   3.8026  10.0583  18.7704  -4.3610  \n",
       "172632  21.1988   2.1164   8.4392   9.7136  14.5550  -0.3629  \n",
       "...         ...      ...      ...      ...      ...      ...  \n",
       "174810  15.7055  -0.0573  -0.4335   8.9254  15.5573  18.2979  \n",
       "183503  22.3925   1.6333  -2.3253   8.5062  13.7761 -10.7626  \n",
       "112986  17.5139  -2.1711   8.4400   8.4767  10.3667   7.9010  \n",
       "163611  16.8322  -2.2593  -4.0647   8.2448  19.3697   1.3178  \n",
       "15021   21.0034   0.1780   9.3603   7.8386  17.0758  -8.7806  \n",
       "\n",
       "[50000 rows x 201 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fTest = fTest.sample(50000)\n",
    "fTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6ecf5be-88a0-4f57-867e-5a5053c1b541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_code</th>\n",
       "      <th>target</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>...</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>118259</th>\n",
       "      <td>train_118259</td>\n",
       "      <td>1</td>\n",
       "      <td>11.6550</td>\n",
       "      <td>0.4861</td>\n",
       "      <td>11.2349</td>\n",
       "      <td>7.0910</td>\n",
       "      <td>8.2756</td>\n",
       "      <td>-9.2809</td>\n",
       "      <td>7.0930</td>\n",
       "      <td>14.4174</td>\n",
       "      <td>...</td>\n",
       "      <td>7.5596</td>\n",
       "      <td>4.7456</td>\n",
       "      <td>1.4361</td>\n",
       "      <td>-0.2309</td>\n",
       "      <td>19.1999</td>\n",
       "      <td>-4.3001</td>\n",
       "      <td>12.0954</td>\n",
       "      <td>8.6657</td>\n",
       "      <td>16.8992</td>\n",
       "      <td>-21.1892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147227</th>\n",
       "      <td>train_147227</td>\n",
       "      <td>0</td>\n",
       "      <td>9.6934</td>\n",
       "      <td>0.3825</td>\n",
       "      <td>10.7220</td>\n",
       "      <td>8.4255</td>\n",
       "      <td>14.3667</td>\n",
       "      <td>-20.3207</td>\n",
       "      <td>4.6158</td>\n",
       "      <td>17.5395</td>\n",
       "      <td>...</td>\n",
       "      <td>2.5536</td>\n",
       "      <td>7.1667</td>\n",
       "      <td>1.6063</td>\n",
       "      <td>-4.2859</td>\n",
       "      <td>16.6339</td>\n",
       "      <td>-1.7381</td>\n",
       "      <td>1.8723</td>\n",
       "      <td>8.8940</td>\n",
       "      <td>14.1444</td>\n",
       "      <td>-5.1713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116569</th>\n",
       "      <td>train_116569</td>\n",
       "      <td>0</td>\n",
       "      <td>9.7640</td>\n",
       "      <td>-1.9364</td>\n",
       "      <td>8.8070</td>\n",
       "      <td>7.9485</td>\n",
       "      <td>8.9377</td>\n",
       "      <td>-19.0602</td>\n",
       "      <td>5.2902</td>\n",
       "      <td>9.5768</td>\n",
       "      <td>...</td>\n",
       "      <td>9.1211</td>\n",
       "      <td>6.5127</td>\n",
       "      <td>4.2783</td>\n",
       "      <td>8.1762</td>\n",
       "      <td>13.2334</td>\n",
       "      <td>0.0964</td>\n",
       "      <td>-0.9409</td>\n",
       "      <td>8.5414</td>\n",
       "      <td>19.2781</td>\n",
       "      <td>-5.7867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177652</th>\n",
       "      <td>train_177652</td>\n",
       "      <td>1</td>\n",
       "      <td>2.8771</td>\n",
       "      <td>-1.1922</td>\n",
       "      <td>7.7620</td>\n",
       "      <td>7.3653</td>\n",
       "      <td>9.4814</td>\n",
       "      <td>-3.9042</td>\n",
       "      <td>6.0964</td>\n",
       "      <td>17.7681</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0504</td>\n",
       "      <td>6.0245</td>\n",
       "      <td>2.5681</td>\n",
       "      <td>-7.0409</td>\n",
       "      <td>21.0559</td>\n",
       "      <td>0.4692</td>\n",
       "      <td>-8.9568</td>\n",
       "      <td>9.8416</td>\n",
       "      <td>18.5873</td>\n",
       "      <td>3.7217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50125</th>\n",
       "      <td>train_50125</td>\n",
       "      <td>0</td>\n",
       "      <td>7.1818</td>\n",
       "      <td>-4.9192</td>\n",
       "      <td>8.8219</td>\n",
       "      <td>4.2660</td>\n",
       "      <td>8.4297</td>\n",
       "      <td>-10.4704</td>\n",
       "      <td>4.2622</td>\n",
       "      <td>9.5381</td>\n",
       "      <td>...</td>\n",
       "      <td>8.6249</td>\n",
       "      <td>5.3034</td>\n",
       "      <td>4.8084</td>\n",
       "      <td>4.9776</td>\n",
       "      <td>19.8397</td>\n",
       "      <td>-0.3409</td>\n",
       "      <td>5.9617</td>\n",
       "      <td>9.6181</td>\n",
       "      <td>17.6123</td>\n",
       "      <td>9.4325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107496</th>\n",
       "      <td>train_107496</td>\n",
       "      <td>0</td>\n",
       "      <td>12.7165</td>\n",
       "      <td>0.2826</td>\n",
       "      <td>12.2517</td>\n",
       "      <td>6.0281</td>\n",
       "      <td>9.1144</td>\n",
       "      <td>-11.7688</td>\n",
       "      <td>3.7977</td>\n",
       "      <td>15.9558</td>\n",
       "      <td>...</td>\n",
       "      <td>4.3549</td>\n",
       "      <td>3.4759</td>\n",
       "      <td>4.2780</td>\n",
       "      <td>4.6315</td>\n",
       "      <td>14.2056</td>\n",
       "      <td>-0.5516</td>\n",
       "      <td>2.7464</td>\n",
       "      <td>8.6361</td>\n",
       "      <td>19.1092</td>\n",
       "      <td>10.5627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44012</th>\n",
       "      <td>train_44012</td>\n",
       "      <td>0</td>\n",
       "      <td>12.2637</td>\n",
       "      <td>-4.1245</td>\n",
       "      <td>4.7019</td>\n",
       "      <td>8.1311</td>\n",
       "      <td>10.4074</td>\n",
       "      <td>5.4277</td>\n",
       "      <td>5.6654</td>\n",
       "      <td>14.3779</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0169</td>\n",
       "      <td>6.1938</td>\n",
       "      <td>1.6797</td>\n",
       "      <td>6.4043</td>\n",
       "      <td>21.4733</td>\n",
       "      <td>0.9707</td>\n",
       "      <td>9.4860</td>\n",
       "      <td>9.1864</td>\n",
       "      <td>12.8181</td>\n",
       "      <td>-2.1350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67743</th>\n",
       "      <td>train_67743</td>\n",
       "      <td>1</td>\n",
       "      <td>15.3188</td>\n",
       "      <td>-2.8258</td>\n",
       "      <td>10.4232</td>\n",
       "      <td>8.4900</td>\n",
       "      <td>13.4405</td>\n",
       "      <td>-10.4193</td>\n",
       "      <td>3.9426</td>\n",
       "      <td>16.6276</td>\n",
       "      <td>...</td>\n",
       "      <td>12.1711</td>\n",
       "      <td>5.7385</td>\n",
       "      <td>2.8303</td>\n",
       "      <td>-6.1098</td>\n",
       "      <td>21.5730</td>\n",
       "      <td>0.3496</td>\n",
       "      <td>1.4911</td>\n",
       "      <td>8.7962</td>\n",
       "      <td>11.5357</td>\n",
       "      <td>-6.5625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38745</th>\n",
       "      <td>train_38745</td>\n",
       "      <td>0</td>\n",
       "      <td>10.7509</td>\n",
       "      <td>-4.0081</td>\n",
       "      <td>13.2728</td>\n",
       "      <td>4.8501</td>\n",
       "      <td>10.3089</td>\n",
       "      <td>0.7325</td>\n",
       "      <td>6.9126</td>\n",
       "      <td>15.5001</td>\n",
       "      <td>...</td>\n",
       "      <td>3.8412</td>\n",
       "      <td>6.6443</td>\n",
       "      <td>5.0993</td>\n",
       "      <td>-0.7895</td>\n",
       "      <td>21.5675</td>\n",
       "      <td>-0.2374</td>\n",
       "      <td>-7.2017</td>\n",
       "      <td>9.2637</td>\n",
       "      <td>14.7508</td>\n",
       "      <td>-6.0230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28488</th>\n",
       "      <td>train_28488</td>\n",
       "      <td>0</td>\n",
       "      <td>8.5453</td>\n",
       "      <td>-3.4036</td>\n",
       "      <td>11.9952</td>\n",
       "      <td>9.6250</td>\n",
       "      <td>11.6830</td>\n",
       "      <td>1.9560</td>\n",
       "      <td>6.3126</td>\n",
       "      <td>15.0214</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.4800</td>\n",
       "      <td>7.5988</td>\n",
       "      <td>2.6066</td>\n",
       "      <td>1.0653</td>\n",
       "      <td>12.0027</td>\n",
       "      <td>0.5038</td>\n",
       "      <td>-3.4012</td>\n",
       "      <td>9.5776</td>\n",
       "      <td>15.2114</td>\n",
       "      <td>9.3641</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID_code  target    var_0   var_1    var_2   var_3    var_4  \\\n",
       "118259  train_118259       1  11.6550  0.4861  11.2349  7.0910   8.2756   \n",
       "147227  train_147227       0   9.6934  0.3825  10.7220  8.4255  14.3667   \n",
       "116569  train_116569       0   9.7640 -1.9364   8.8070  7.9485   8.9377   \n",
       "177652  train_177652       1   2.8771 -1.1922   7.7620  7.3653   9.4814   \n",
       "50125    train_50125       0   7.1818 -4.9192   8.8219  4.2660   8.4297   \n",
       "...              ...     ...      ...     ...      ...     ...      ...   \n",
       "107496  train_107496       0  12.7165  0.2826  12.2517  6.0281   9.1144   \n",
       "44012    train_44012       0  12.2637 -4.1245   4.7019  8.1311  10.4074   \n",
       "67743    train_67743       1  15.3188 -2.8258  10.4232  8.4900  13.4405   \n",
       "38745    train_38745       0  10.7509 -4.0081  13.2728  4.8501  10.3089   \n",
       "28488    train_28488       0   8.5453 -3.4036  11.9952  9.6250  11.6830   \n",
       "\n",
       "          var_5   var_6    var_7  ...  var_190  var_191  var_192  var_193  \\\n",
       "118259  -9.2809  7.0930  14.4174  ...   7.5596   4.7456   1.4361  -0.2309   \n",
       "147227 -20.3207  4.6158  17.5395  ...   2.5536   7.1667   1.6063  -4.2859   \n",
       "116569 -19.0602  5.2902   9.5768  ...   9.1211   6.5127   4.2783   8.1762   \n",
       "177652  -3.9042  6.0964  17.7681  ...   6.0504   6.0245   2.5681  -7.0409   \n",
       "50125  -10.4704  4.2622   9.5381  ...   8.6249   5.3034   4.8084   4.9776   \n",
       "...         ...     ...      ...  ...      ...      ...      ...      ...   \n",
       "107496 -11.7688  3.7977  15.9558  ...   4.3549   3.4759   4.2780   4.6315   \n",
       "44012    5.4277  5.6654  14.3779  ...   3.0169   6.1938   1.6797   6.4043   \n",
       "67743  -10.4193  3.9426  16.6276  ...  12.1711   5.7385   2.8303  -6.1098   \n",
       "38745    0.7325  6.9126  15.5001  ...   3.8412   6.6443   5.0993  -0.7895   \n",
       "28488    1.9560  6.3126  15.0214  ...  -4.4800   7.5988   2.6066   1.0653   \n",
       "\n",
       "        var_194  var_195  var_196  var_197  var_198  var_199  \n",
       "118259  19.1999  -4.3001  12.0954   8.6657  16.8992 -21.1892  \n",
       "147227  16.6339  -1.7381   1.8723   8.8940  14.1444  -5.1713  \n",
       "116569  13.2334   0.0964  -0.9409   8.5414  19.2781  -5.7867  \n",
       "177652  21.0559   0.4692  -8.9568   9.8416  18.5873   3.7217  \n",
       "50125   19.8397  -0.3409   5.9617   9.6181  17.6123   9.4325  \n",
       "...         ...      ...      ...      ...      ...      ...  \n",
       "107496  14.2056  -0.5516   2.7464   8.6361  19.1092  10.5627  \n",
       "44012   21.4733   0.9707   9.4860   9.1864  12.8181  -2.1350  \n",
       "67743   21.5730   0.3496   1.4911   8.7962  11.5357  -6.5625  \n",
       "38745   21.5675  -0.2374  -7.2017   9.2637  14.7508  -6.0230  \n",
       "28488   12.0027   0.5038  -3.4012   9.5776  15.2114   9.3641  \n",
       "\n",
       "[50000 rows x 202 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTrain = dfTrain.sample(50000)\n",
    "dfTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e9515b1-11bb-4d6b-90a0-e27e3e6884dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40000, 200), (10000, 200), (40000,), (10000,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_columns=[c for c in dfTrain.columns if c not in ['ID_code', 'target']]\n",
    "X = dfTrain.loc[:, var_columns]\n",
    "y=dfTrain.loc[:, 'target']\n",
    "X_train, X_valid, y_train, y_valid = train_test_split (X,y, test_size=.2)\n",
    "X_train.shape, X_valid.shape, y_train.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3e0c4f7-3f3d-4ea9-8272-527a9c092780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective': 'binary:logistic',\n",
       " 'use_label_encoder': True,\n",
       " 'base_score': None,\n",
       " 'booster': None,\n",
       " 'colsample_bylevel': None,\n",
       " 'colsample_bynode': None,\n",
       " 'colsample_bytree': None,\n",
       " 'enable_categorical': False,\n",
       " 'gamma': None,\n",
       " 'gpu_id': None,\n",
       " 'importance_type': None,\n",
       " 'interaction_constraints': None,\n",
       " 'learning_rate': None,\n",
       " 'max_delta_step': None,\n",
       " 'max_depth': None,\n",
       " 'min_child_weight': None,\n",
       " 'missing': nan,\n",
       " 'monotone_constraints': None,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'num_parallel_tree': None,\n",
       " 'predictor': None,\n",
       " 'random_state': None,\n",
       " 'reg_alpha': None,\n",
       " 'reg_lambda': None,\n",
       " 'scale_pos_weight': None,\n",
       " 'subsample': None,\n",
       " 'tree_method': None,\n",
       " 'validate_parameters': None,\n",
       " 'verbosity': None}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.XGBClassifier().get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ee5e134-51c8-4276-a68a-0755d7f9a77f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lakshmi\\anaconda3\\envs\\dev\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.60965\n",
      "[1]\tvalidation_0-auc:0.64315\n",
      "[2]\tvalidation_0-auc:0.65248\n",
      "[3]\tvalidation_0-auc:0.67070\n",
      "[4]\tvalidation_0-auc:0.68515\n",
      "[5]\tvalidation_0-auc:0.69722\n",
      "[6]\tvalidation_0-auc:0.70181\n",
      "[7]\tvalidation_0-auc:0.70306\n",
      "[8]\tvalidation_0-auc:0.70483\n",
      "[9]\tvalidation_0-auc:0.70920\n",
      "[10]\tvalidation_0-auc:0.70779\n",
      "[11]\tvalidation_0-auc:0.70597\n",
      "[12]\tvalidation_0-auc:0.70855\n",
      "[13]\tvalidation_0-auc:0.71093\n",
      "[14]\tvalidation_0-auc:0.71494\n",
      "[15]\tvalidation_0-auc:0.71672\n",
      "[16]\tvalidation_0-auc:0.71739\n",
      "[17]\tvalidation_0-auc:0.71899\n",
      "[18]\tvalidation_0-auc:0.72547\n",
      "[19]\tvalidation_0-auc:0.72935\n",
      "[20]\tvalidation_0-auc:0.72981\n",
      "[21]\tvalidation_0-auc:0.73339\n",
      "[22]\tvalidation_0-auc:0.73492\n",
      "[23]\tvalidation_0-auc:0.73469\n",
      "[24]\tvalidation_0-auc:0.73598\n",
      "[25]\tvalidation_0-auc:0.73645\n",
      "[26]\tvalidation_0-auc:0.73717\n",
      "[27]\tvalidation_0-auc:0.73767\n",
      "[28]\tvalidation_0-auc:0.73739\n",
      "[29]\tvalidation_0-auc:0.73780\n",
      "[30]\tvalidation_0-auc:0.73959\n",
      "[31]\tvalidation_0-auc:0.73899\n",
      "[32]\tvalidation_0-auc:0.73981\n",
      "[33]\tvalidation_0-auc:0.74332\n",
      "[34]\tvalidation_0-auc:0.74560\n",
      "[35]\tvalidation_0-auc:0.74774\n",
      "[36]\tvalidation_0-auc:0.74857\n",
      "[37]\tvalidation_0-auc:0.75172\n",
      "[38]\tvalidation_0-auc:0.75199\n",
      "[39]\tvalidation_0-auc:0.75276\n",
      "[40]\tvalidation_0-auc:0.75270\n",
      "[41]\tvalidation_0-auc:0.75261\n",
      "[42]\tvalidation_0-auc:0.75367\n",
      "[43]\tvalidation_0-auc:0.75448\n",
      "[44]\tvalidation_0-auc:0.75457\n",
      "[45]\tvalidation_0-auc:0.75532\n",
      "[46]\tvalidation_0-auc:0.75526\n",
      "[47]\tvalidation_0-auc:0.75566\n",
      "[48]\tvalidation_0-auc:0.75752\n",
      "[49]\tvalidation_0-auc:0.75756\n",
      "[50]\tvalidation_0-auc:0.75680\n",
      "[51]\tvalidation_0-auc:0.75733\n",
      "[52]\tvalidation_0-auc:0.75775\n",
      "[53]\tvalidation_0-auc:0.75710\n",
      "[54]\tvalidation_0-auc:0.75828\n",
      "[55]\tvalidation_0-auc:0.75846\n",
      "[56]\tvalidation_0-auc:0.75865\n",
      "[57]\tvalidation_0-auc:0.75929\n",
      "[58]\tvalidation_0-auc:0.75898\n",
      "[59]\tvalidation_0-auc:0.76062\n",
      "[60]\tvalidation_0-auc:0.76230\n",
      "[61]\tvalidation_0-auc:0.76197\n",
      "[62]\tvalidation_0-auc:0.76192\n",
      "[63]\tvalidation_0-auc:0.76305\n",
      "[64]\tvalidation_0-auc:0.76335\n",
      "[65]\tvalidation_0-auc:0.76402\n",
      "[66]\tvalidation_0-auc:0.76498\n",
      "[67]\tvalidation_0-auc:0.76492\n",
      "[68]\tvalidation_0-auc:0.76498\n",
      "[69]\tvalidation_0-auc:0.76455\n",
      "[70]\tvalidation_0-auc:0.76414\n",
      "[71]\tvalidation_0-auc:0.76329\n",
      "[72]\tvalidation_0-auc:0.76388\n",
      "[73]\tvalidation_0-auc:0.76498\n",
      "[74]\tvalidation_0-auc:0.76619\n",
      "[75]\tvalidation_0-auc:0.76744\n",
      "[76]\tvalidation_0-auc:0.76753\n",
      "[77]\tvalidation_0-auc:0.76841\n",
      "[78]\tvalidation_0-auc:0.76836\n",
      "[79]\tvalidation_0-auc:0.76816\n",
      "[80]\tvalidation_0-auc:0.76831\n",
      "[81]\tvalidation_0-auc:0.76770\n",
      "[82]\tvalidation_0-auc:0.76891\n",
      "[83]\tvalidation_0-auc:0.76903\n",
      "[84]\tvalidation_0-auc:0.76963\n",
      "[85]\tvalidation_0-auc:0.76964\n",
      "[86]\tvalidation_0-auc:0.76953\n",
      "[87]\tvalidation_0-auc:0.76861\n",
      "[88]\tvalidation_0-auc:0.76893\n",
      "[89]\tvalidation_0-auc:0.76942\n",
      "[90]\tvalidation_0-auc:0.76965\n",
      "[91]\tvalidation_0-auc:0.77046\n",
      "[92]\tvalidation_0-auc:0.77084\n",
      "[93]\tvalidation_0-auc:0.77147\n",
      "[94]\tvalidation_0-auc:0.77209\n",
      "[95]\tvalidation_0-auc:0.77263\n",
      "[96]\tvalidation_0-auc:0.77279\n",
      "[97]\tvalidation_0-auc:0.77293\n",
      "[98]\tvalidation_0-auc:0.77240\n",
      "[99]\tvalidation_0-auc:0.77243\n",
      "[100]\tvalidation_0-auc:0.77194\n",
      "[101]\tvalidation_0-auc:0.77253\n",
      "[102]\tvalidation_0-auc:0.77358\n",
      "[103]\tvalidation_0-auc:0.77335\n",
      "[104]\tvalidation_0-auc:0.77351\n",
      "[105]\tvalidation_0-auc:0.77379\n",
      "[106]\tvalidation_0-auc:0.77535\n",
      "[107]\tvalidation_0-auc:0.77531\n",
      "[108]\tvalidation_0-auc:0.77563\n",
      "[109]\tvalidation_0-auc:0.77599\n",
      "[110]\tvalidation_0-auc:0.77598\n",
      "[111]\tvalidation_0-auc:0.77711\n",
      "[112]\tvalidation_0-auc:0.77678\n",
      "[113]\tvalidation_0-auc:0.77704\n",
      "[114]\tvalidation_0-auc:0.77708\n",
      "[115]\tvalidation_0-auc:0.77711\n",
      "[116]\tvalidation_0-auc:0.77668\n",
      "[117]\tvalidation_0-auc:0.77746\n",
      "[118]\tvalidation_0-auc:0.77681\n",
      "[119]\tvalidation_0-auc:0.77686\n",
      "[120]\tvalidation_0-auc:0.77792\n",
      "[121]\tvalidation_0-auc:0.77804\n",
      "[122]\tvalidation_0-auc:0.77823\n",
      "[123]\tvalidation_0-auc:0.77844\n",
      "[124]\tvalidation_0-auc:0.77831\n",
      "[125]\tvalidation_0-auc:0.77879\n",
      "[126]\tvalidation_0-auc:0.77879\n",
      "[127]\tvalidation_0-auc:0.77862\n",
      "[128]\tvalidation_0-auc:0.77877\n",
      "[129]\tvalidation_0-auc:0.77873\n",
      "[130]\tvalidation_0-auc:0.77922\n",
      "[131]\tvalidation_0-auc:0.77917\n",
      "[132]\tvalidation_0-auc:0.77923\n",
      "[133]\tvalidation_0-auc:0.78004\n",
      "[134]\tvalidation_0-auc:0.77932\n",
      "[135]\tvalidation_0-auc:0.77955\n",
      "[136]\tvalidation_0-auc:0.78004\n",
      "[137]\tvalidation_0-auc:0.78024\n",
      "[138]\tvalidation_0-auc:0.77988\n",
      "[139]\tvalidation_0-auc:0.77951\n",
      "[140]\tvalidation_0-auc:0.77929\n",
      "[141]\tvalidation_0-auc:0.77961\n",
      "[142]\tvalidation_0-auc:0.78019\n",
      "[143]\tvalidation_0-auc:0.78017\n",
      "[144]\tvalidation_0-auc:0.78075\n",
      "[145]\tvalidation_0-auc:0.78129\n",
      "[146]\tvalidation_0-auc:0.78165\n",
      "[147]\tvalidation_0-auc:0.78212\n",
      "[148]\tvalidation_0-auc:0.78149\n",
      "[149]\tvalidation_0-auc:0.78133\n",
      "[150]\tvalidation_0-auc:0.78164\n",
      "[151]\tvalidation_0-auc:0.78218\n",
      "[152]\tvalidation_0-auc:0.78210\n",
      "[153]\tvalidation_0-auc:0.78281\n",
      "[154]\tvalidation_0-auc:0.78239\n",
      "[155]\tvalidation_0-auc:0.78287\n",
      "[156]\tvalidation_0-auc:0.78274\n",
      "[157]\tvalidation_0-auc:0.78290\n",
      "[158]\tvalidation_0-auc:0.78289\n",
      "[159]\tvalidation_0-auc:0.78317\n",
      "[160]\tvalidation_0-auc:0.78361\n",
      "[161]\tvalidation_0-auc:0.78383\n",
      "[162]\tvalidation_0-auc:0.78427\n",
      "[163]\tvalidation_0-auc:0.78466\n",
      "[164]\tvalidation_0-auc:0.78457\n",
      "[165]\tvalidation_0-auc:0.78504\n",
      "[166]\tvalidation_0-auc:0.78571\n",
      "[167]\tvalidation_0-auc:0.78581\n",
      "[168]\tvalidation_0-auc:0.78597\n",
      "[169]\tvalidation_0-auc:0.78587\n",
      "[170]\tvalidation_0-auc:0.78567\n",
      "[171]\tvalidation_0-auc:0.78556\n",
      "[172]\tvalidation_0-auc:0.78574\n",
      "[173]\tvalidation_0-auc:0.78612\n",
      "[174]\tvalidation_0-auc:0.78597\n",
      "[175]\tvalidation_0-auc:0.78608\n",
      "[176]\tvalidation_0-auc:0.78660\n",
      "[177]\tvalidation_0-auc:0.78645\n",
      "[178]\tvalidation_0-auc:0.78663\n",
      "[179]\tvalidation_0-auc:0.78722\n",
      "[180]\tvalidation_0-auc:0.78726\n",
      "[181]\tvalidation_0-auc:0.78723\n",
      "[182]\tvalidation_0-auc:0.78756\n",
      "[183]\tvalidation_0-auc:0.78757\n",
      "[184]\tvalidation_0-auc:0.78814\n",
      "[185]\tvalidation_0-auc:0.78819\n",
      "[186]\tvalidation_0-auc:0.78824\n",
      "[187]\tvalidation_0-auc:0.78827\n",
      "[188]\tvalidation_0-auc:0.78844\n",
      "[189]\tvalidation_0-auc:0.78882\n",
      "[190]\tvalidation_0-auc:0.78870\n",
      "[191]\tvalidation_0-auc:0.78863\n",
      "[192]\tvalidation_0-auc:0.78895\n",
      "[193]\tvalidation_0-auc:0.78953\n",
      "[194]\tvalidation_0-auc:0.79010\n",
      "[195]\tvalidation_0-auc:0.79002\n",
      "[196]\tvalidation_0-auc:0.79015\n",
      "[197]\tvalidation_0-auc:0.79022\n",
      "[198]\tvalidation_0-auc:0.79009\n",
      "[199]\tvalidation_0-auc:0.78972\n",
      "[200]\tvalidation_0-auc:0.78982\n",
      "[201]\tvalidation_0-auc:0.78986\n",
      "[202]\tvalidation_0-auc:0.78996\n",
      "[203]\tvalidation_0-auc:0.79030\n",
      "[204]\tvalidation_0-auc:0.79043\n",
      "[205]\tvalidation_0-auc:0.79012\n",
      "[206]\tvalidation_0-auc:0.79060\n",
      "[207]\tvalidation_0-auc:0.79099\n",
      "[208]\tvalidation_0-auc:0.79125\n",
      "[209]\tvalidation_0-auc:0.79135\n",
      "[210]\tvalidation_0-auc:0.79161\n",
      "[211]\tvalidation_0-auc:0.79172\n",
      "[212]\tvalidation_0-auc:0.79181\n",
      "[213]\tvalidation_0-auc:0.79200\n",
      "[214]\tvalidation_0-auc:0.79176\n",
      "[215]\tvalidation_0-auc:0.79188\n",
      "[216]\tvalidation_0-auc:0.79192\n",
      "[217]\tvalidation_0-auc:0.79198\n",
      "[218]\tvalidation_0-auc:0.79223\n",
      "[219]\tvalidation_0-auc:0.79270\n",
      "[220]\tvalidation_0-auc:0.79293\n",
      "[221]\tvalidation_0-auc:0.79330\n",
      "[222]\tvalidation_0-auc:0.79339\n",
      "[223]\tvalidation_0-auc:0.79345\n",
      "[224]\tvalidation_0-auc:0.79355\n",
      "[225]\tvalidation_0-auc:0.79368\n",
      "[226]\tvalidation_0-auc:0.79349\n",
      "[227]\tvalidation_0-auc:0.79399\n",
      "[228]\tvalidation_0-auc:0.79403\n",
      "[229]\tvalidation_0-auc:0.79398\n",
      "[230]\tvalidation_0-auc:0.79475\n",
      "[231]\tvalidation_0-auc:0.79510\n",
      "[232]\tvalidation_0-auc:0.79501\n",
      "[233]\tvalidation_0-auc:0.79513\n",
      "[234]\tvalidation_0-auc:0.79541\n",
      "[235]\tvalidation_0-auc:0.79542\n",
      "[236]\tvalidation_0-auc:0.79551\n",
      "[237]\tvalidation_0-auc:0.79540\n",
      "[238]\tvalidation_0-auc:0.79545\n",
      "[239]\tvalidation_0-auc:0.79607\n",
      "[240]\tvalidation_0-auc:0.79632\n",
      "[241]\tvalidation_0-auc:0.79631\n",
      "[242]\tvalidation_0-auc:0.79653\n",
      "[243]\tvalidation_0-auc:0.79673\n",
      "[244]\tvalidation_0-auc:0.79701\n",
      "[245]\tvalidation_0-auc:0.79713\n",
      "[246]\tvalidation_0-auc:0.79772\n",
      "[247]\tvalidation_0-auc:0.79814\n",
      "[248]\tvalidation_0-auc:0.79817\n",
      "[249]\tvalidation_0-auc:0.79821\n",
      "[250]\tvalidation_0-auc:0.79808\n",
      "[251]\tvalidation_0-auc:0.79847\n",
      "[252]\tvalidation_0-auc:0.79836\n",
      "[253]\tvalidation_0-auc:0.79875\n",
      "[254]\tvalidation_0-auc:0.79873\n",
      "[255]\tvalidation_0-auc:0.79915\n",
      "[256]\tvalidation_0-auc:0.79923\n",
      "[257]\tvalidation_0-auc:0.79954\n",
      "[258]\tvalidation_0-auc:0.79964\n",
      "[259]\tvalidation_0-auc:0.79985\n",
      "[260]\tvalidation_0-auc:0.79973\n",
      "[261]\tvalidation_0-auc:0.80020\n",
      "[262]\tvalidation_0-auc:0.80002\n",
      "[263]\tvalidation_0-auc:0.80037\n",
      "[264]\tvalidation_0-auc:0.80073\n",
      "[265]\tvalidation_0-auc:0.80069\n",
      "[266]\tvalidation_0-auc:0.80116\n",
      "[267]\tvalidation_0-auc:0.80161\n",
      "[268]\tvalidation_0-auc:0.80192\n",
      "[269]\tvalidation_0-auc:0.80235\n",
      "[270]\tvalidation_0-auc:0.80253\n",
      "[271]\tvalidation_0-auc:0.80278\n",
      "[272]\tvalidation_0-auc:0.80282\n",
      "[273]\tvalidation_0-auc:0.80297\n",
      "[274]\tvalidation_0-auc:0.80317\n",
      "[275]\tvalidation_0-auc:0.80287\n",
      "[276]\tvalidation_0-auc:0.80307\n",
      "[277]\tvalidation_0-auc:0.80316\n",
      "[278]\tvalidation_0-auc:0.80342\n",
      "[279]\tvalidation_0-auc:0.80378\n",
      "[280]\tvalidation_0-auc:0.80375\n",
      "[281]\tvalidation_0-auc:0.80410\n",
      "[282]\tvalidation_0-auc:0.80415\n",
      "[283]\tvalidation_0-auc:0.80465\n",
      "[284]\tvalidation_0-auc:0.80524\n",
      "[285]\tvalidation_0-auc:0.80529\n",
      "[286]\tvalidation_0-auc:0.80562\n",
      "[287]\tvalidation_0-auc:0.80624\n",
      "[288]\tvalidation_0-auc:0.80625\n",
      "[289]\tvalidation_0-auc:0.80660\n",
      "[290]\tvalidation_0-auc:0.80673\n",
      "[291]\tvalidation_0-auc:0.80710\n",
      "[292]\tvalidation_0-auc:0.80683\n",
      "[293]\tvalidation_0-auc:0.80702\n",
      "[294]\tvalidation_0-auc:0.80699\n",
      "[295]\tvalidation_0-auc:0.80741\n",
      "[296]\tvalidation_0-auc:0.80742\n",
      "[297]\tvalidation_0-auc:0.80731\n",
      "[298]\tvalidation_0-auc:0.80739\n",
      "[299]\tvalidation_0-auc:0.80773\n",
      "[300]\tvalidation_0-auc:0.80789\n",
      "[301]\tvalidation_0-auc:0.80844\n",
      "[302]\tvalidation_0-auc:0.80886\n",
      "[303]\tvalidation_0-auc:0.80915\n",
      "[304]\tvalidation_0-auc:0.80928\n",
      "[305]\tvalidation_0-auc:0.80943\n",
      "[306]\tvalidation_0-auc:0.80950\n",
      "[307]\tvalidation_0-auc:0.80965\n",
      "[308]\tvalidation_0-auc:0.80976\n",
      "[309]\tvalidation_0-auc:0.80983\n",
      "[310]\tvalidation_0-auc:0.81008\n",
      "[311]\tvalidation_0-auc:0.81039\n",
      "[312]\tvalidation_0-auc:0.81088\n",
      "[313]\tvalidation_0-auc:0.81094\n",
      "[314]\tvalidation_0-auc:0.81127\n",
      "[315]\tvalidation_0-auc:0.81165\n",
      "[316]\tvalidation_0-auc:0.81154\n",
      "[317]\tvalidation_0-auc:0.81146\n",
      "[318]\tvalidation_0-auc:0.81174\n",
      "[319]\tvalidation_0-auc:0.81197\n",
      "[320]\tvalidation_0-auc:0.81215\n",
      "[321]\tvalidation_0-auc:0.81207\n",
      "[322]\tvalidation_0-auc:0.81228\n",
      "[323]\tvalidation_0-auc:0.81261\n",
      "[324]\tvalidation_0-auc:0.81288\n",
      "[325]\tvalidation_0-auc:0.81274\n",
      "[326]\tvalidation_0-auc:0.81293\n",
      "[327]\tvalidation_0-auc:0.81288\n",
      "[328]\tvalidation_0-auc:0.81296\n",
      "[329]\tvalidation_0-auc:0.81301\n",
      "[330]\tvalidation_0-auc:0.81288\n",
      "[331]\tvalidation_0-auc:0.81303\n",
      "[332]\tvalidation_0-auc:0.81321\n",
      "[333]\tvalidation_0-auc:0.81348\n",
      "[334]\tvalidation_0-auc:0.81346\n",
      "[335]\tvalidation_0-auc:0.81378\n",
      "[336]\tvalidation_0-auc:0.81372\n",
      "[337]\tvalidation_0-auc:0.81369\n",
      "[338]\tvalidation_0-auc:0.81422\n",
      "[339]\tvalidation_0-auc:0.81429\n",
      "[340]\tvalidation_0-auc:0.81454\n",
      "[341]\tvalidation_0-auc:0.81468\n",
      "[342]\tvalidation_0-auc:0.81474\n",
      "[343]\tvalidation_0-auc:0.81485\n",
      "[344]\tvalidation_0-auc:0.81480\n",
      "[345]\tvalidation_0-auc:0.81490\n",
      "[346]\tvalidation_0-auc:0.81535\n",
      "[347]\tvalidation_0-auc:0.81547\n",
      "[348]\tvalidation_0-auc:0.81559\n",
      "[349]\tvalidation_0-auc:0.81588\n",
      "[350]\tvalidation_0-auc:0.81585\n",
      "[351]\tvalidation_0-auc:0.81602\n",
      "[352]\tvalidation_0-auc:0.81609\n",
      "[353]\tvalidation_0-auc:0.81612\n",
      "[354]\tvalidation_0-auc:0.81619\n",
      "[355]\tvalidation_0-auc:0.81640\n",
      "[356]\tvalidation_0-auc:0.81657\n",
      "[357]\tvalidation_0-auc:0.81692\n",
      "[358]\tvalidation_0-auc:0.81715\n",
      "[359]\tvalidation_0-auc:0.81736\n",
      "[360]\tvalidation_0-auc:0.81760\n",
      "[361]\tvalidation_0-auc:0.81756\n",
      "[362]\tvalidation_0-auc:0.81789\n",
      "[363]\tvalidation_0-auc:0.81813\n",
      "[364]\tvalidation_0-auc:0.81814\n",
      "[365]\tvalidation_0-auc:0.81827\n",
      "[366]\tvalidation_0-auc:0.81836\n",
      "[367]\tvalidation_0-auc:0.81869\n",
      "[368]\tvalidation_0-auc:0.81876\n",
      "[369]\tvalidation_0-auc:0.81894\n",
      "[370]\tvalidation_0-auc:0.81912\n",
      "[371]\tvalidation_0-auc:0.81949\n",
      "[372]\tvalidation_0-auc:0.81985\n",
      "[373]\tvalidation_0-auc:0.81995\n",
      "[374]\tvalidation_0-auc:0.82026\n",
      "[375]\tvalidation_0-auc:0.82043\n",
      "[376]\tvalidation_0-auc:0.82061\n",
      "[377]\tvalidation_0-auc:0.82076\n",
      "[378]\tvalidation_0-auc:0.82081\n",
      "[379]\tvalidation_0-auc:0.82093\n",
      "[380]\tvalidation_0-auc:0.82100\n",
      "[381]\tvalidation_0-auc:0.82132\n",
      "[382]\tvalidation_0-auc:0.82132\n",
      "[383]\tvalidation_0-auc:0.82121\n",
      "[384]\tvalidation_0-auc:0.82139\n",
      "[385]\tvalidation_0-auc:0.82151\n",
      "[386]\tvalidation_0-auc:0.82179\n",
      "[387]\tvalidation_0-auc:0.82201\n",
      "[388]\tvalidation_0-auc:0.82220\n",
      "[389]\tvalidation_0-auc:0.82240\n",
      "[390]\tvalidation_0-auc:0.82250\n",
      "[391]\tvalidation_0-auc:0.82247\n",
      "[392]\tvalidation_0-auc:0.82250\n",
      "[393]\tvalidation_0-auc:0.82259\n",
      "[394]\tvalidation_0-auc:0.82271\n",
      "[395]\tvalidation_0-auc:0.82297\n",
      "[396]\tvalidation_0-auc:0.82300\n",
      "[397]\tvalidation_0-auc:0.82312\n",
      "[398]\tvalidation_0-auc:0.82333\n",
      "[399]\tvalidation_0-auc:0.82357\n",
      "[400]\tvalidation_0-auc:0.82364\n",
      "[401]\tvalidation_0-auc:0.82370\n",
      "[402]\tvalidation_0-auc:0.82368\n",
      "[403]\tvalidation_0-auc:0.82359\n",
      "[404]\tvalidation_0-auc:0.82390\n",
      "[405]\tvalidation_0-auc:0.82407\n",
      "[406]\tvalidation_0-auc:0.82420\n",
      "[407]\tvalidation_0-auc:0.82439\n",
      "[408]\tvalidation_0-auc:0.82418\n",
      "[409]\tvalidation_0-auc:0.82429\n",
      "[410]\tvalidation_0-auc:0.82439\n",
      "[411]\tvalidation_0-auc:0.82433\n",
      "[412]\tvalidation_0-auc:0.82454\n",
      "[413]\tvalidation_0-auc:0.82468\n",
      "[414]\tvalidation_0-auc:0.82494\n",
      "[415]\tvalidation_0-auc:0.82512\n",
      "[416]\tvalidation_0-auc:0.82521\n",
      "[417]\tvalidation_0-auc:0.82515\n",
      "[418]\tvalidation_0-auc:0.82530\n",
      "[419]\tvalidation_0-auc:0.82555\n",
      "[420]\tvalidation_0-auc:0.82582\n",
      "[421]\tvalidation_0-auc:0.82597\n",
      "[422]\tvalidation_0-auc:0.82620\n",
      "[423]\tvalidation_0-auc:0.82638\n",
      "[424]\tvalidation_0-auc:0.82654\n",
      "[425]\tvalidation_0-auc:0.82675\n",
      "[426]\tvalidation_0-auc:0.82680\n",
      "[427]\tvalidation_0-auc:0.82703\n",
      "[428]\tvalidation_0-auc:0.82702\n",
      "[429]\tvalidation_0-auc:0.82724\n",
      "[430]\tvalidation_0-auc:0.82759\n",
      "[431]\tvalidation_0-auc:0.82773\n",
      "[432]\tvalidation_0-auc:0.82774\n",
      "[433]\tvalidation_0-auc:0.82777\n",
      "[434]\tvalidation_0-auc:0.82763\n",
      "[435]\tvalidation_0-auc:0.82777\n",
      "[436]\tvalidation_0-auc:0.82796\n",
      "[437]\tvalidation_0-auc:0.82813\n",
      "[438]\tvalidation_0-auc:0.82831\n",
      "[439]\tvalidation_0-auc:0.82854\n",
      "[440]\tvalidation_0-auc:0.82858\n",
      "[441]\tvalidation_0-auc:0.82867\n",
      "[442]\tvalidation_0-auc:0.82883\n",
      "[443]\tvalidation_0-auc:0.82907\n",
      "[444]\tvalidation_0-auc:0.82909\n",
      "[445]\tvalidation_0-auc:0.82928\n",
      "[446]\tvalidation_0-auc:0.82930\n",
      "[447]\tvalidation_0-auc:0.82935\n",
      "[448]\tvalidation_0-auc:0.82935\n",
      "[449]\tvalidation_0-auc:0.82952\n",
      "[450]\tvalidation_0-auc:0.82970\n",
      "[451]\tvalidation_0-auc:0.82970\n",
      "[452]\tvalidation_0-auc:0.82974\n",
      "[453]\tvalidation_0-auc:0.82993\n",
      "[454]\tvalidation_0-auc:0.82999\n",
      "[455]\tvalidation_0-auc:0.82996\n",
      "[456]\tvalidation_0-auc:0.83006\n",
      "[457]\tvalidation_0-auc:0.83007\n",
      "[458]\tvalidation_0-auc:0.83022\n",
      "[459]\tvalidation_0-auc:0.83043\n",
      "[460]\tvalidation_0-auc:0.83036\n",
      "[461]\tvalidation_0-auc:0.83031\n",
      "[462]\tvalidation_0-auc:0.83037\n",
      "[463]\tvalidation_0-auc:0.83054\n",
      "[464]\tvalidation_0-auc:0.83050\n",
      "[465]\tvalidation_0-auc:0.83059\n",
      "[466]\tvalidation_0-auc:0.83062\n",
      "[467]\tvalidation_0-auc:0.83064\n",
      "[468]\tvalidation_0-auc:0.83097\n",
      "[469]\tvalidation_0-auc:0.83114\n",
      "[470]\tvalidation_0-auc:0.83113\n",
      "[471]\tvalidation_0-auc:0.83122\n",
      "[472]\tvalidation_0-auc:0.83133\n",
      "[473]\tvalidation_0-auc:0.83138\n",
      "[474]\tvalidation_0-auc:0.83168\n",
      "[475]\tvalidation_0-auc:0.83179\n",
      "[476]\tvalidation_0-auc:0.83192\n",
      "[477]\tvalidation_0-auc:0.83202\n",
      "[478]\tvalidation_0-auc:0.83203\n",
      "[479]\tvalidation_0-auc:0.83225\n",
      "[480]\tvalidation_0-auc:0.83243\n",
      "[481]\tvalidation_0-auc:0.83253\n",
      "[482]\tvalidation_0-auc:0.83247\n",
      "[483]\tvalidation_0-auc:0.83255\n",
      "[484]\tvalidation_0-auc:0.83260\n",
      "[485]\tvalidation_0-auc:0.83278\n",
      "[486]\tvalidation_0-auc:0.83277\n",
      "[487]\tvalidation_0-auc:0.83310\n",
      "[488]\tvalidation_0-auc:0.83310\n",
      "[489]\tvalidation_0-auc:0.83312\n",
      "[490]\tvalidation_0-auc:0.83330\n",
      "[491]\tvalidation_0-auc:0.83359\n",
      "[492]\tvalidation_0-auc:0.83368\n",
      "[493]\tvalidation_0-auc:0.83370\n",
      "[494]\tvalidation_0-auc:0.83379\n",
      "[495]\tvalidation_0-auc:0.83400\n",
      "[496]\tvalidation_0-auc:0.83413\n",
      "[497]\tvalidation_0-auc:0.83428\n",
      "[498]\tvalidation_0-auc:0.83440\n",
      "[499]\tvalidation_0-auc:0.83461\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.5,\n",
       "              enable_categorical=False, eval_metric='auc', gamma=0, gpu_id=-1,\n",
       "              importance_type=None, interaction_constraints='',\n",
       "              learning_rate=0.01, max_delta_step=0, max_depth=5,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=500, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
       "              random_state=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "              subsample=0.5, tree_method='exact', validate_parameters=1,\n",
       "              verbosity=1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_xgboost = xgb.XGBClassifier(learning_rate=.01,\n",
    "                                     max_depth=5,\n",
    "                                     n_estimators=500,\n",
    "                                     subsample=.5,\n",
    "                                     colsample_bytree=.5,\n",
    "                                     eval_metric='auc',\n",
    "                                     verbosity=1,\n",
    "                                    random_state=1)\n",
    "\n",
    "eval_set =[(X_valid, y_valid)]\n",
    "\n",
    "model_xgboost.fit(X_train,\n",
    "                 y_train,\n",
    "                 early_stopping_rounds=10,\n",
    "                 eval_set=eval_set,\n",
    "                 verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e004e4-841e-44ff-923f-ade4a55c313c",
   "metadata": {},
   "source": [
    "# Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3db96e3e-5b9b-4c9a-9f22-4bf3a94d9916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Train: 0.9348\n",
      "AUC Valid 0.8346\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = model_xgboost.predict_proba(X_train)[:,1]\n",
    "y_valid_pred = model_xgboost.predict_proba(X_valid)[:,1]\n",
    "\n",
    "print(\"AUC Train: {:.4f}\\nAUC Valid {:.4f}\".format(roc_auc_score(y_train, y_train_pred), roc_auc_score(y_valid,y_valid_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667e9393-c8a4-414e-99ec-b2ddd777c096",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c89bafe9-c054-4655-8507-2d2133f7ceb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'learning_rate': [0.05, 0.1, 0.15],\n",
       " 'njobs': [2, 4, 6],\n",
       " 'baseScore': [0.3, 0.5, 0.7]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = [0.05, .1, .15]\n",
    "n_jobs = [2, 4, 6]\n",
    "base_score = [0.3,0.5,0.7]\n",
    "\n",
    "params_dict = {\"learning_rate\": learning_rate,\n",
    "              \"njobs\":n_jobs,\n",
    "              \"baseScore\":base_score}\n",
    "\n",
    "num_combinations = 1\n",
    "for v in params_dict.values(): num_combinations *=len(v)\n",
    "    \n",
    "print (num_combinations)\n",
    "params_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3237a4c3-5b0c-4cd9-8293-b7e86bda2322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 27 candidates, totalling 54 fits\n",
      "[15:16:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"baseScore\", \"njobs\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 1/2] END .....baseScore=0.3, learning_rate=0.05, njobs=2; total time=  25.3s\n",
      "[15:17:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"baseScore\", \"njobs\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 2/2] END .....baseScore=0.3, learning_rate=0.05, njobs=2; total time=  24.6s\n",
      "[15:17:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"baseScore\", \"njobs\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 1/2] END .....baseScore=0.3, learning_rate=0.05, njobs=4; total time=  23.2s\n",
      "[15:18:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"baseScore\", \"njobs\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 2/2] END .....baseScore=0.3, learning_rate=0.05, njobs=4; total time=  25.1s\n",
      "[15:18:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"baseScore\", \"njobs\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 1/2] END .....baseScore=0.3, learning_rate=0.05, njobs=6; total time=  24.9s\n",
      "[15:19:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"baseScore\", \"njobs\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 2/2] END .....baseScore=0.3, learning_rate=0.05, njobs=6; total time=  46.4s\n",
      "[15:19:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"baseScore\", \"njobs\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 1/2] END ......baseScore=0.3, learning_rate=0.1, njobs=2; total time=  29.6s\n",
      "[15:20:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"baseScore\", \"njobs\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 2/2] END ......baseScore=0.3, learning_rate=0.1, njobs=2; total time=  32.7s\n",
      "[15:20:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"baseScore\", \"njobs\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 1/2] END ......baseScore=0.3, learning_rate=0.1, njobs=4; total time=  30.2s\n",
      "[15:21:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"baseScore\", \"njobs\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 2/2] END ......baseScore=0.3, learning_rate=0.1, njobs=4; total time=  27.6s\n",
      "[15:21:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"baseScore\", \"njobs\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 1/2] END ......baseScore=0.3, learning_rate=0.1, njobs=6; total time=  29.3s\n",
      "[15:22:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"baseScore\", \"njobs\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 2/2] END ......baseScore=0.3, learning_rate=0.1, njobs=6; total time=  27.3s\n",
      "[15:22:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"baseScore\", \"njobs\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 1/2] END .....baseScore=0.3, learning_rate=0.15, njobs=2; total time=  34.5s\n",
      "[15:23:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"baseScore\", \"njobs\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 2/2] END .....baseScore=0.3, learning_rate=0.15, njobs=2; total time=  27.1s\n",
      "[15:23:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"baseScore\", \"njobs\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 1/2] END .....baseScore=0.3, learning_rate=0.15, njobs=4; total time=  23.8s\n",
      "[15:24:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"baseScore\", \"njobs\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 2/2] END .....baseScore=0.3, learning_rate=0.15, njobs=4; total time=  23.0s\n",
      "[15:24:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"baseScore\", \"njobs\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 1/2] END .....baseScore=0.3, learning_rate=0.15, njobs=6; total time=  32.1s\n",
      "[15:25:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"baseScore\", \"njobs\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 2/2] END .....baseScore=0.3, learning_rate=0.15, njobs=6; total time=  29.7s\n",
      "[15:25:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"baseScore\", \"njobs\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 1/2] END .....baseScore=0.5, learning_rate=0.05, njobs=2; total time=  24.4s\n",
      "[15:26:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"baseScore\", \"njobs\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 2/2] END .....baseScore=0.5, learning_rate=0.05, njobs=2; total time=  23.6s\n",
      "[15:26:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"baseScore\", \"njobs\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 1/2] END .....baseScore=0.5, learning_rate=0.05, njobs=4; total time=  21.6s\n",
      "[15:26:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"baseScore\", \"njobs\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 2/2] END .....baseScore=0.5, learning_rate=0.05, njobs=4; total time=  21.6s\n",
      "[15:27:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"baseScore\", \"njobs\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 1/2] END .....baseScore=0.5, learning_rate=0.05, njobs=6; total time=  26.6s\n",
      "[15:27:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"baseScore\", \"njobs\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 2/2] END .....baseScore=0.5, learning_rate=0.05, njobs=6; total time=  23.8s\n",
      "[15:28:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"baseScore\", \"njobs\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 1/2] END ......baseScore=0.5, learning_rate=0.1, njobs=2; total time=  23.5s\n",
      "[15:28:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"baseScore\", \"njobs\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 2/2] END ......baseScore=0.5, learning_rate=0.1, njobs=2; total time=  25.4s\n",
      "[15:28:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"baseScore\", \"njobs\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 1/2] END ......baseScore=0.5, learning_rate=0.1, njobs=4; total time=  47.7s\n",
      "[15:29:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"baseScore\", \"njobs\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 2/2] END ......baseScore=0.5, learning_rate=0.1, njobs=4; total time=  27.5s\n",
      "[15:30:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"baseScore\", \"njobs\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 1/2] END ......baseScore=0.5, learning_rate=0.1, njobs=6; total time=  23.1s\n",
      "[15:30:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"baseScore\", \"njobs\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 2/2] END ......baseScore=0.5, learning_rate=0.1, njobs=6; total time=  24.5s\n",
      "[15:30:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"baseScore\", \"njobs\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 1/2] END .....baseScore=0.5, learning_rate=0.15, njobs=2; total time=  33.8s\n",
      "[15:31:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"baseScore\", \"njobs\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 2/2] END .....baseScore=0.5, learning_rate=0.15, njobs=2; total time=  23.9s\n",
      "[15:31:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"baseScore\", \"njobs\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 1/2] END .....baseScore=0.5, learning_rate=0.15, njobs=4; total time=  25.3s\n",
      "[15:32:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"baseScore\", \"njobs\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 2/2] END .....baseScore=0.5, learning_rate=0.15, njobs=4; total time=  30.4s\n",
      "[15:32:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"baseScore\", \"njobs\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 1/2] END .....baseScore=0.5, learning_rate=0.15, njobs=6; total time=  32.1s\n",
      "[15:33:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"baseScore\", \"njobs\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 2/2] END .....baseScore=0.5, learning_rate=0.15, njobs=6; total time=  28.7s\n",
      "[15:33:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"baseScore\", \"njobs\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 1/2] END .....baseScore=0.7, learning_rate=0.05, njobs=2; total time=  28.7s\n",
      "[15:34:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"baseScore\", \"njobs\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 2/2] END .....baseScore=0.7, learning_rate=0.05, njobs=2; total time=  26.9s\n",
      "[15:34:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"baseScore\", \"njobs\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 1/2] END .....baseScore=0.7, learning_rate=0.05, njobs=4; total time=  22.7s\n",
      "[15:35:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"baseScore\", \"njobs\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 2/2] END .....baseScore=0.7, learning_rate=0.05, njobs=4; total time=  22.0s\n",
      "[15:35:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"baseScore\", \"njobs\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 1/2] END .....baseScore=0.7, learning_rate=0.05, njobs=6; total time=  24.2s\n",
      "[15:36:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"baseScore\", \"njobs\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 2/2] END .....baseScore=0.7, learning_rate=0.05, njobs=6; total time=  24.6s\n",
      "[15:36:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"baseScore\", \"njobs\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 1/2] END ......baseScore=0.7, learning_rate=0.1, njobs=2; total time=  23.8s\n",
      "[15:36:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"baseScore\", \"njobs\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 2/2] END ......baseScore=0.7, learning_rate=0.1, njobs=2; total time=  28.7s\n",
      "[15:37:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"baseScore\", \"njobs\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 1/2] END ......baseScore=0.7, learning_rate=0.1, njobs=4; total time=  30.6s\n",
      "[15:37:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"baseScore\", \"njobs\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 2/2] END ......baseScore=0.7, learning_rate=0.1, njobs=4; total time=  27.5s\n",
      "[15:38:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"baseScore\", \"njobs\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 1/2] END ......baseScore=0.7, learning_rate=0.1, njobs=6; total time=  29.3s\n",
      "[15:38:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"baseScore\", \"njobs\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 2/2] END ......baseScore=0.7, learning_rate=0.1, njobs=6; total time=  26.1s\n",
      "[15:39:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"baseScore\", \"njobs\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 1/2] END .....baseScore=0.7, learning_rate=0.15, njobs=2; total time=  27.0s\n",
      "[15:39:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"baseScore\", \"njobs\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 2/2] END .....baseScore=0.7, learning_rate=0.15, njobs=2; total time=  26.3s\n",
      "[15:40:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"baseScore\", \"njobs\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 1/2] END .....baseScore=0.7, learning_rate=0.15, njobs=4; total time=  32.7s\n",
      "[15:40:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"baseScore\", \"njobs\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 2/2] END .....baseScore=0.7, learning_rate=0.15, njobs=4; total time=  25.8s\n",
      "[15:41:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"baseScore\", \"njobs\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 1/2] END .....baseScore=0.7, learning_rate=0.15, njobs=6; total time=  28.3s\n",
      "[15:41:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"baseScore\", \"njobs\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 2/2] END .....baseScore=0.7, learning_rate=0.15, njobs=6; total time=  26.1s\n",
      "[15:42:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"baseScore\", \"njobs\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2,\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=0.25,\n",
       "                                     enable_categorical=False,\n",
       "                                     eval_metric='auc', gamma=None, gpu_id=None,\n",
       "                                     importance_type=None,\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None, max_delta_step=None,\n",
       "                                     max_depth=None, min_child_weight=None,\n",
       "                                     missing=nan, monotone...\n",
       "                                     num_parallel_tree=None, predictor=None,\n",
       "                                     random_state=None, reg_alpha=None,\n",
       "                                     reg_lambda=None, scale_pos_weight=None,\n",
       "                                     subsample=0.5, tree_method=None,\n",
       "                                     use_label_encoder=False,\n",
       "                                     validate_parameters=None, verbosity=None),\n",
       "             param_grid={'baseScore': [0.3, 0.5, 0.7],\n",
       "                         'learning_rate': [0.05, 0.1, 0.15],\n",
       "                         'njobs': [2, 4, 6]},\n",
       "             return_train_score=True,\n",
       "             scoring=<function my_roc_auc_score at 0x0000022F80712558>,\n",
       "             verbose=4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_roc_auc_score(model, X, y): return roc_auc_score(y, model.predict_proba(X)[:,1])\n",
    "\n",
    "model_xgboost_hp = GridSearchCV(estimator=xgb.XGBClassifier(subsample=0.5,\n",
    "                                                               colsample_bytree=0.25,\n",
    "                                                               eval_metric='auc',\n",
    "                                                               use_label_encoder=False),\n",
    "                               param_grid=params_dict,\n",
    "                               cv=2,\n",
    "                               scoring=my_roc_auc_score,\n",
    "                               return_train_score=True,\n",
    "                               verbose=4)\n",
    "model_xgboost_hp.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eeb889b3-fff6-4ec2-b9c8-8c3c5bd02abe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_njobs</th>\n",
       "      <th>param_baseScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>0.828252</td>\n",
       "      <td>0.990762</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>0.828252</td>\n",
       "      <td>0.990762</td>\n",
       "      <td>0.1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.828252</td>\n",
       "      <td>0.990762</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.828252</td>\n",
       "      <td>0.990762</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0.828252</td>\n",
       "      <td>0.990762</td>\n",
       "      <td>0.1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>0.828252</td>\n",
       "      <td>0.990762</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>0.828252</td>\n",
       "      <td>0.990762</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>0.828252</td>\n",
       "      <td>0.990762</td>\n",
       "      <td>0.1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>0.828252</td>\n",
       "      <td>0.990762</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>10</td>\n",
       "      <td>0.824388</td>\n",
       "      <td>0.996532</td>\n",
       "      <td>0.15</td>\n",
       "      <td>2</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10</td>\n",
       "      <td>0.824388</td>\n",
       "      <td>0.996532</td>\n",
       "      <td>0.15</td>\n",
       "      <td>6</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10</td>\n",
       "      <td>0.824388</td>\n",
       "      <td>0.996532</td>\n",
       "      <td>0.15</td>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10</td>\n",
       "      <td>0.824388</td>\n",
       "      <td>0.996532</td>\n",
       "      <td>0.15</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>10</td>\n",
       "      <td>0.824388</td>\n",
       "      <td>0.996532</td>\n",
       "      <td>0.15</td>\n",
       "      <td>6</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>0.824388</td>\n",
       "      <td>0.996532</td>\n",
       "      <td>0.15</td>\n",
       "      <td>6</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>0.824388</td>\n",
       "      <td>0.996532</td>\n",
       "      <td>0.15</td>\n",
       "      <td>4</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>0.824388</td>\n",
       "      <td>0.996532</td>\n",
       "      <td>0.15</td>\n",
       "      <td>2</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10</td>\n",
       "      <td>0.824388</td>\n",
       "      <td>0.996532</td>\n",
       "      <td>0.15</td>\n",
       "      <td>4</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>19</td>\n",
       "      <td>0.821997</td>\n",
       "      <td>0.972647</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>19</td>\n",
       "      <td>0.821997</td>\n",
       "      <td>0.972647</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>19</td>\n",
       "      <td>0.821997</td>\n",
       "      <td>0.972647</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>0.821997</td>\n",
       "      <td>0.972647</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0.821997</td>\n",
       "      <td>0.972647</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>19</td>\n",
       "      <td>0.821997</td>\n",
       "      <td>0.972647</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19</td>\n",
       "      <td>0.821997</td>\n",
       "      <td>0.972647</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>0.821997</td>\n",
       "      <td>0.972647</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>0.821997</td>\n",
       "      <td>0.972647</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    rank_test_score  mean_test_score  mean_train_score param_learning_rate  \\\n",
       "13                1         0.828252          0.990762                 0.1   \n",
       "23                1         0.828252          0.990762                 0.1   \n",
       "3                 1         0.828252          0.990762                 0.1   \n",
       "4                 1         0.828252          0.990762                 0.1   \n",
       "5                 1         0.828252          0.990762                 0.1   \n",
       "22                1         0.828252          0.990762                 0.1   \n",
       "21                1         0.828252          0.990762                 0.1   \n",
       "14                1         0.828252          0.990762                 0.1   \n",
       "12                1         0.828252          0.990762                 0.1   \n",
       "24               10         0.824388          0.996532                0.15   \n",
       "17               10         0.824388          0.996532                0.15   \n",
       "16               10         0.824388          0.996532                0.15   \n",
       "15               10         0.824388          0.996532                0.15   \n",
       "26               10         0.824388          0.996532                0.15   \n",
       "8                10         0.824388          0.996532                0.15   \n",
       "7                10         0.824388          0.996532                0.15   \n",
       "6                10         0.824388          0.996532                0.15   \n",
       "25               10         0.824388          0.996532                0.15   \n",
       "11               19         0.821997          0.972647                0.05   \n",
       "10               19         0.821997          0.972647                0.05   \n",
       "9                19         0.821997          0.972647                0.05   \n",
       "18               19         0.821997          0.972647                0.05   \n",
       "19               19         0.821997          0.972647                0.05   \n",
       "20               19         0.821997          0.972647                0.05   \n",
       "2                19         0.821997          0.972647                0.05   \n",
       "1                19         0.821997          0.972647                0.05   \n",
       "0                19         0.821997          0.972647                0.05   \n",
       "\n",
       "   param_njobs param_baseScore  \n",
       "13           4             0.5  \n",
       "23           6             0.7  \n",
       "3            2             0.3  \n",
       "4            4             0.3  \n",
       "5            6             0.3  \n",
       "22           4             0.7  \n",
       "21           2             0.7  \n",
       "14           6             0.5  \n",
       "12           2             0.5  \n",
       "24           2             0.7  \n",
       "17           6             0.5  \n",
       "16           4             0.5  \n",
       "15           2             0.5  \n",
       "26           6             0.7  \n",
       "8            6             0.3  \n",
       "7            4             0.3  \n",
       "6            2             0.3  \n",
       "25           4             0.7  \n",
       "11           6             0.5  \n",
       "10           4             0.5  \n",
       "9            2             0.5  \n",
       "18           2             0.7  \n",
       "19           4             0.7  \n",
       "20           6             0.7  \n",
       "2            6             0.3  \n",
       "1            4             0.3  \n",
       "0            2             0.3  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cv_results1 = pd.DataFrame(model_xgboost_hp.cv_results_)\n",
    "df_cv_results1 = df_cv_results1[['rank_test_score','mean_test_score', 'mean_train_score','param_learning_rate','param_njobs',\n",
    "                              'param_baseScore']]\n",
    "\n",
    "df_cv_results1.sort_values(by='rank_test_score', inplace=True)\n",
    "df_cv_results1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39ab7a80-6546-4db1-8823-6e0e5838924f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv_results1.to_csv(\"output/model1_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c2d8f3a2-9bb5-4534-ae7f-fd1c7a278ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Affect of Learning Rate on Performance')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_cv_results1.sort_values(by='param_learning_rate', inplace=True)\n",
    "\n",
    "# Find values of AUC for learning rate of 0.05 and different values of depth\n",
    "lr_t3k_d2 = df_cv_results1.loc[(df_cv_results1['param_baseScore']==.7) & (df_cv_results1['param_njobs']==2),:]\n",
    "\n",
    "# Let us plot now\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "lr_t3k_d2.plot(x='param_learning_rate', y='mean_test_score', label='baseScore=.7, n_jobs=2', ax=ax)\n",
    "plt.ylabel('Mean Test Score')\n",
    "plt.title('Affect of Learning Rate on Performance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3779d9-927d-4091-86b9-bd2a22d8e07f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448a9156-d084-4c2d-9153-a6327ada8c84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
